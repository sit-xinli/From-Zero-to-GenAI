from langchain_community.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.tools import DuckDuckGoSearchResults
from langchain.memory import ConversationBufferMemory
from langchain.agents import initialize_agent, Tool
import time

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸš€ **Initializing ChatOpenAI LLM**
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# - Model: gpt-3.5-turbo
# - Temperature: 0.7 (balanced creativity)
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)

print("------------------------------------START------------------------------------\n")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ–¼ï¸âœ¨ **Multi-Modal - Image Description**
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("------ ğŸ¤–ğŸ”§ **Multi-Modal - Image Description** ------\n")
# Simulating an image description task.
image_description = "A picture of a cat sitting on a sofa."
text_chain = LLMChain(llm=llm, prompt=PromptTemplate(input_variables=["description"], template="Describe this image: {description}"))
image_analysis = text_chain.run(image_description)
print(f"Image Description: {image_analysis}\n")
print("------ ğŸ¤–ğŸ”§ **Multi-Modal - Image Description** ------\n")




# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸŒğŸ” **LLM Integration - Search**
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("------ ğŸ¤–ğŸ”§ **LLM Integration - Search** ------\n")
# Using DuckDuckGo to fetch search results.
search = DuckDuckGoSearchResults()
search_results = search.run("LangChain documentation")
print(f"Search Results: {search_results}\n")
print("------ ğŸ¤–ğŸ”§ **LLM Integration - Search** ------\n")




# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ”„ğŸ“„ **Data Augmentation - Text Variations**
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("------ ğŸ¤–ğŸ”§ **Data Augmentation - Text Variations** ------\n")
# Generating paraphrased variations of sentences.
original_sentences = ["The weather is great today.", "It's a sunny day."]
augmented_data = [
    llm.invoke([{"role": "user", "content": f"Paraphrase: {sentence}"}]).content
    for sentence in original_sentences
]
print(f"Augmented Data: {augmented_data}\n")
print("------ ğŸ¤–ğŸ”§ **Data Augmentation - Text Variations** ------\n")




# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸš¨âš™ï¸ **Webhooks and Event Handling**
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("------ ğŸ¤–ğŸ”§ **Webhooks and Event Handling** ------\n")
# Handling event-based tasks.
def trigger_event(event):
    if event == "new_query":
        return "Event: A new query has been received, processing..."
    return "Event: Unknown."

# Simulating an event trigger
event_response = trigger_event("new_query")
print(f"Event Response: {event_response}\n")
print("------ ğŸ¤–ğŸ”§ **Webhooks and Event Handling** ------\n")





# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ¤ğŸ¶ **Streaming - Real-time Model Output**
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Start of streaming the modelâ€™s response token by token.
print("Streaming Model Response: ", end="", flush=True)
for token in llm.stream([{"role": "user", "content": "Tell me a joke"}]):
    print(f"{token.content}", end="", flush=True)  # Continue printing tokens on the same line
    time.sleep(0.5)

print("\n\n------ ğŸ¤–ğŸ”§ **End of Streaming - Real-time Model Output** ------\n")



print("------------------------------------END------------------------------------")



# Group 3:
# Multi-Modal: Integrating text and image data.
# LLM Integrations: Combining LLMs with external tools (e.g., search engines).
# Data Augmentation: Generating variations of input text.
# Evaluation: Assessing the quality of generated output.
# Webhooks: Handling event-driven tasks with webhooks.
# Streaming: Streaming tokens in real-time from LLMs
